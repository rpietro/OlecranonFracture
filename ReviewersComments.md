## **Reviewers' comments:**

### Reviewer #2: This research studied the stability and reliability of four different classification of olecranon fracture by comparing the kappa coefficient of inter- and intra-observer. I read this paper with great interest, but think that the manuscript **has great flaw for publication of CORR**. 

The study protocol was consisted of eight surgeons and 18 fractures. 
1.Firstly, I would like to know how they decided the numbers of participating surgeons and fractures. 
**R: Ricardo, como justificamos isso no artigo?**

2.Secondly, dynamic imaging (image of lateral view in flexion and extension, active and passive) is essential for grading in some classification, i would like to ask how they determine the classification is such cases. 
**R:** 

Thirdly, the author should mention about the relationship between the classification and the final results.
**R: A relação entre as classificações e os resultados finais foram discutidos.**


### Reviewer #3: This is an original paper comparing the classification systems used in the literature. It has not been done before to my knowledge and therefore will add to the present literature. I like the inter and intraobserver reliability. This adds validity to the study. 

1.Abstract: Under the purpose, I would include any hypothesis you may have had going in to the paper. Did you think one classification system would fare better than the others and for what reason?
**R:Our hypothesis is that Colton Classification presents an acceptable agreement since it is simpler to use, on the other hand considering AO Classification´s complexity we expect it to reach a lower level of agreement.**

2. I would label line 7 beginning with "Radiographic images…." With the Subtitle: Methods and Materials. Line 14, label "Results", line 19 label "Conclusion"
**R: Feito**

Images
Line 53: please define the profile view. Is this a lateral view of the elbow?
**R: Added. Eighteen cases of elbow joint fractures with anteroposterior and profile (lateral view of the elbow) radiographic images were selected from the records of the Hospital Municipal São José (Joinville - SC).**

What were the dates (years?) the study subjects were retrieved from? 
**R: Querem saber quando as imagens foram retrieved?
The images were retrieved from August to December 2012**

How many sets of studies were evaluated before choosing the final 18 studies? 
**R: Ricardo, não sei quantas foram avaliadas previamente.**

Were only 2 views included in the evaluation process? 
**R: Yes**

Is this the "standard" series in your institution and how does it compare to other institutions when evaluating and classifying elbow radiographs? 
**R:**

Please further define the inclusion and exclusion criteria for choosing the subjects to evaluate. 
**R: Abaixo está como descrevemos no artigo. Não usamos nenhum critério de inclusão.**
*Our study included eight orthopedic surgeons who work in the same department. Orthopedic surgeons were split into two groups, with four participants each, one with specialists in upper extremity surgery and the other with orthopedic surgeons without a specific focus on upper extremity surgery.*

Line 55: Who chose the images used in the study? 
**R: Added. The images were chosen by two orthopedic residents and one orthopedic surgeon specialist in upper extremety surgery who were aware of the classification systems.**

Was the person choosing the images aware of the classification systems in order to choose a good representation of fracture types? Please comment on this in the discussion section.
**R:**

Could this have an effect on the results? 
**R:**

**Any bias on the part of the person choosing the radiographs to be evaluated?
**R:**

Line 57: Please explain what you mean by "degree of contamination." 
**R: Ricardo, você poderia explicar?**
*Any signs that could lead to patient identification were removed. The **degree of contamination** in case of exposed fractures and the final outcome were not disclosed to the evaluators. Radiographic images with incorrect olecranon position, that could cause any misunderstanding in image classification were excluded. Images with low quality or with artifacts or other technical defects during image acquisition were also excluded.* 

How did you decide on 10 sets of radiographs for the final analysis? 
**R:**

Is this a large enough number to reach statistical significance within each of the classification systems? 
**R:**

Procedures
Please comment on whether the moderator classified these radiographs ahead of time. And if so, as a resident with less experience than an upper extremity specialist, how accurate was this?
**R: As 8 imagens do pré-teste eram discutidas depois que cada ortopedista classificava as imagens.**

If the radiographs were not classified ahead of time, then how was a consensus reached? 
**R: Chegava-se a um consenso através de discussão.**

Did everyone agree or was there a majority of agreement? 
**R: There was a majority of agreement.**

How accurate do you feel this method is? 
**R:**

Was the moderator involved in choosing the xrays?
**R: Sim.**

Prior to the second evaluation, was the training session repeated?
**R: Não.**

Discussion
I appreciate that the authors recognize that the Colton classification system fared the best as it is what is widely used at their institution. If the study were to be repeated at an institution that used the AO classification system or at Mayo where that classification system is used, would the results have been markedly different? This may be a fatal flaw of the paper. 
**R:**

In addition, I am not sure you can claim that the Colton classification system is used worldwide. It does not appear to be one widely cited in the US literature. This statement is misleading. 
**R: No nosso artigo tem essa frase e posso deletar o **known worldwide**: *The Colton classification is simple, known worldwide, descriptive and the prognosis is indicated according to injury pattern.*

Lines 190-193: I do agree that by choosing only the "best" radiographs that this may artificially improve overall agreement. This does not reflect a true representation of patients with fractures mas they are often difficult to position. Why did you not pick a time frame and evaluate all subjects and classify?
**R:**
